{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, LayerNormalization\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "from tensorflow.image import extract_patches\n",
    "from tensorflow.keras.layers import Conv2D, Layer, Dense, Embedding\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, LayerNormalization\n",
    "from tensorflow.keras.activations import softmax\n",
    "from math import ceil\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import cv2\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, LayerNormalization\n",
    "from tensorflow.keras.activations import softmax\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f214fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path_(inputs, drop_prob, is_training):\n",
    "    \n",
    "    # Bypass in non-training mode\n",
    "    if (not is_training) or (drop_prob == 0.):\n",
    "        return inputs\n",
    "\n",
    "    # Compute keep_prob\n",
    "    keep_prob = 1.0 - drop_prob\n",
    "\n",
    "    # Compute drop_connect tensor\n",
    "    input_shape = tf.shape(inputs)\n",
    "    batch_num = input_shape[0]; rank = len(input_shape)\n",
    "    \n",
    "    shape = (batch_num,) + (1,) * (rank - 1)\n",
    "    random_tensor = keep_prob + tf.random.uniform(shape, dtype=inputs.dtype)\n",
    "    path_mask = tf.floor(random_tensor)\n",
    "    output = tf.math.divide(inputs, keep_prob) * path_mask\n",
    "    return output\n",
    "\n",
    "class drop_path(Layer):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return drop_path_(x, self.drop_prob, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62232e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, eps = 1e-5):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(shape = (input_shape[-1],), dtype = tf.float32, trainable = True, initializer = tf.keras.initializers.Constant(1.), name = 'weight')\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    def call(self, inputs):\n",
    "        stddev = tf.math.maximum(tf.math.sqrt(tf.math.reduce_mean(inputs ** 2, axis = -1, keepdims = True)), self.eps)\n",
    "        results = inputs / stddev\n",
    "        results = results * self.weight\n",
    "        return results\n",
    "    def get_config(self):\n",
    "        config = super(RMSNorm, self).get_config()\n",
    "        config['eps'] = self.eps\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "def selective_scan(u, delta, A, B, C, D):\n",
    "    dA = tf.einsum('bld,dn->bldn', delta, A) # first step of A_bar = exp(ΔA), i.e., ΔA\n",
    "    dB_u = tf.einsum('bld,bld,bln->bldn', delta, u, B)\n",
    "    dA_cumsum = tf.pad(\n",
    "        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n",
    "    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1\n",
    "    # Cumulative sum along all the input tokens, parallel prefix sum, calculates dA for all the input tokens parallely\n",
    "    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  \n",
    "    dA_cumsum = tf.exp(dA_cumsum)  # second step of A_bar = exp(ΔA), i.e., exp(ΔA)\n",
    "    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1\n",
    "    x = dB_u * dA_cumsum\n",
    "    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) # 1e-12 to avoid division by 0\n",
    "    y = tf.einsum('bldn,bln->bld', x, C)\n",
    "    return y + u * D \n",
    "\n",
    "class SSM(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, expand = 2, d_state = 16, bias = False):\n",
    "        super(SSM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.expand = expand\n",
    "        self.d_state = d_state\n",
    "        self.bias = bias\n",
    "        self.dt_rank = ceil(self.d_model / 16)\n",
    "    def build(self, input_shape):\n",
    "        self.x_proj_weight = self.add_weight(shape = (self.d_model * self.expand, self.dt_rank + 2 * self.d_state), dtype = tf.float32, trainable = True, name = 'x_proj_weight')\n",
    "        if self.bias:\n",
    "            self.x_proj_bias = self.add_weight(shape = (self.dt_rank + 2 * self.d_state), dtype = tf.float32, trainable = True, name = 'x_proj_bias')\n",
    "        self.dt_proj_weight = self.add_weight(shape = (self.dt_rank, self.expand * self.d_model), dtype = tf.float32, trainable = True, name = 'dt_proj_wei9ght')\n",
    "        self.dt_proj_bias = self.add_weight(shape = (self.expand * self.d_model,), dtype = tf.float32, trainable = True, name = 'dt_proj_bias')\n",
    "        self.A_log = self.add_weight(shape = (self.expand * self.d_model, self.d_state), dtype = tf.float32, trainable = True, name = 'A_log')\n",
    "        self.A_log.assign(tf.math.log(tf.tile(tf.expand_dims(tf.range(1, self.d_state + 1, dtype = tf.float32), axis = 0), (self.expand * self.d_model, 1))))\n",
    "        self.D = self.add_weight(shape = (self.expand * self.d_model,), dtype = tf.float32, trainable = True, initializer = tf.keras.initializers.Constant(1.), name = 'D')\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.d_model * self.expand)\n",
    "    def call(self, x):\n",
    "        # x.shape = (batch, seq_len, d_model * expand)\n",
    "        x_dbl = tf.linalg.matmul(x, self.x_proj_weight) # x_dbl.shape = (batch, seq_len, dt_rank + 2 * d_state)\n",
    "        if self.bias:\n",
    "            x_dbl = x_dbl + self.x_proj_bias\n",
    "        delta, B, C = tf.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], axis = -1)\n",
    "        # delta.shape = (batch, seq_len, dt_rank)\n",
    "        # B.shape = (batch, seq_len, d_state)\n",
    "        # C.shape = (batch, seq_len, d_state)\n",
    "        delta = tf.math.softplus(tf.linalg.matmul(delta, self.dt_proj_weight) + self.dt_proj_bias) # delta.shape = (batch, seq_len, expand * d_model)\n",
    "        # selective scan\n",
    "        # state(t+1) = A state(t) + B x(t) # B is input gate\n",
    "        # y(t)   = C state(t) + D x(t) # C is output gate\n",
    "        A = -tf.exp(self.A_log) # A.shape = (expand * d_model, d_state)\n",
    "        y = selective_scan(x, delta,A,B,C,self.D)\n",
    "        return y\n",
    "    def get_config(self):\n",
    "        config = super(SSM, self).get_config()\n",
    "        config['d_model'] = self.d_model\n",
    "        config['expand'] = self.expand\n",
    "        config['d_state'] = self.d_state\n",
    "        config['bias'] = self.bias\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class MambaBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, expand = 2, bias = False, d_conv = 4, conv_bias = True, d_state = 16):\n",
    "        super(MambaBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.expand = expand\n",
    "        self.d_state = d_state\n",
    "        self.bias = bias\n",
    "        self.dt_rank = ceil(self.d_model / 16)\n",
    "        self.d_conv = d_conv\n",
    "        self.conv_bias = conv_bias\n",
    "        self.fliter = d_model*expand\n",
    "    def call(self,x):\n",
    "        x_and_res = tf.keras.layers.Dense(2 * self.expand * self.d_model, use_bias = self.bias)(x) # results.shape = (batch, seq_len, 2 * expand * d_model)\n",
    "        x, res = tf.keras.layers.Lambda(lambda x: tf.split(x, 2, axis = -1))(x_and_res) # x.shape = (batch, seq_len, expand * d_model)\n",
    "        # spatial & channel mixing\n",
    "        x = tf.keras.layers.Conv1D(self.fliter, kernel_size = self.d_conv, padding = 'same', use_bias = self.conv_bias, activation = tf.keras.activations.swish)(x) # x.shape = (batch, seq_len, expand * d_model)\n",
    "        # selective state space model\n",
    "        y = SSM(self.d_model, self.expand, self.d_state, self.bias)(x) # y.shape = (batch, seq_len, d_model * expand)\n",
    "        # NOTE: borrowing idea of Swish gated linear unit (SwiGLU)\n",
    "        # this layer gates ssm results with swish layer as well. it can be called as swish gated selective state space model (SwiSSM)\n",
    "        y = tf.keras.layers.Lambda(lambda x: x[0] * tf.nn.silu(x[1]))([y, res])\n",
    "        outputs = tf.keras.layers.Dense(self.d_model, use_bias = self.bias)(y)\n",
    "        return outputs\n",
    "\n",
    "def ResidualBlock(d_model, expand = 2, bias = False, d_conv = 4, conv_bias = True, d_state = 16):\n",
    "    inputs = tf.keras.Input((None, d_model)) # inputs.shape = (batch, seq_len, d_model)\n",
    "    Path3 = tf.keras.layers.DepthwiseConv1D(kernel_size=5,strides=1,padding=\"same\")(inputs)\n",
    "    Path3 =  tf.nn.silu(Path3)\n",
    "    results = RMSNorm()(inputs)\n",
    "    path2 = tf.keras.layers.DepthwiseConv1D(kernel_size=3,strides=1,padding=\"same\")(results)\n",
    "    Path2 =  tf.nn.silu(path2)\n",
    "    results = MambaBlock(d_model, expand, bias, d_conv, conv_bias, d_state)(results)\n",
    "    results = tf.keras.layers.Add()([results, inputs, Path3, Path2])\n",
    "    results = tf.keras.layers.Conv1D(d_model, 1, 1, padding=\"same\")(results)\n",
    "    return tf.keras.Model(inputs = inputs, outputs = results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(tf.keras.layers.Layer):\n",
    "    def __init__(self, filter_num, drop=0., name=''):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # MLP layers\n",
    "        self.fc1 = Dense(filter_num[0], name='{}_mlp_0'.format(name))\n",
    "        self.fc2 = Dense(filter_num[1], name='{}_mlp_1'.format(name))\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.drop = Dropout(drop)\n",
    "        \n",
    "        # GELU activation\n",
    "        self.activation = tf.keras.activations.gelu\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        # MLP --> GELU --> Drop --> MLP --> Drop\n",
    "        x = self.fc1(x)\n",
    "        self.activation(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class patch_extract(Layer):\n",
    "    '''\n",
    "    Extract patches from the input feature map.\n",
    "    \n",
    "    patches = patch_extract(patch_size)(feature_map)\n",
    "    \n",
    "    ----------\n",
    "    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, \n",
    "    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. \n",
    "    An image is worth 16x16 words: Transformers for image recognition at scale. \n",
    "    arXiv preprint arXiv:2010.11929.\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "        feature_map: a four-dimensional tensor of (num_sample, width, height, channel)\n",
    "        patch_size: size of split patches (width=height)\n",
    "        \n",
    "    Output\n",
    "    ----------\n",
    "        patches: a two-dimensional tensor of (num_sample*num_patch, patch_size*patch_size)\n",
    "                 where `num_patch = (width // patch_size) * (height // patch_size)`\n",
    "                 \n",
    "    For further information see: https://www.tensorflow.org/api_docs/python/tf/image/extract_patches\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, patch_size):\n",
    "        super(patch_extract, self).__init__()\n",
    "        self.patch_size_x = patch_size[0]\n",
    "        self.patch_size_y = patch_size[1]\n",
    "        \n",
    "    def call(self, images):\n",
    "        \n",
    "        batch_size = tf.shape(images)[0]\n",
    "        \n",
    "        patches = extract_patches(images=images,\n",
    "                                  sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "                                  strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "                                  rates=(1, 1, 1, 1), padding='VALID',)\n",
    "        # patches.shape = (num_sample, patch_num, patch_num, patch_size*channel)\n",
    "        \n",
    "        patch_dim = patches.shape[-1]\n",
    "        patch_numx = patches.shape[1]\n",
    "        patch_numy = patches.shape[2]\n",
    "        patches = tf.reshape(patches, (batch_size, patch_numx*patch_numy, patch_dim))\n",
    "        # patches.shape = (num_sample, patch_num*patch_num, patch_size*channel)\n",
    "        \n",
    "        return patches\n",
    "    \n",
    "class patch_embedding(Layer):\n",
    "    '''\n",
    "    \n",
    "    Embed patches to tokens.\n",
    "    \n",
    "    patches_embed = patch_embedding(num_patch, embed_dim)(pathes)\n",
    "    \n",
    "    ----------\n",
    "    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, \n",
    "    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. \n",
    "    An image is worth 16x16 words: Transformers for image recognition at scale. \n",
    "    arXiv preprint arXiv:2010.11929.\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "        num_patch: number of patches to be embedded.\n",
    "        embed_dim: number of embedded dimensions. \n",
    "        \n",
    "    Output\n",
    "    ----------\n",
    "        embed: Embedded patches.\n",
    "    \n",
    "    For further information see: https://keras.io/api/layers/core_layers/embedding/\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_patch, embed_dim):\n",
    "        super(patch_embedding, self).__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = Dense(embed_dim)\n",
    "        self.pos_embed = Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
    "        embed = self.proj(patch) + self.pos_embed(pos)\n",
    "        return embed\n",
    "\n",
    "class patch_merging(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Downsample embedded patches; it halfs the number of patches\n",
    "    and double the embedded dimensions (c.f. pooling layers).\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "        num_patch: number of patches to be embedded.\n",
    "        embed_dim: number of embedded dimensions. \n",
    "        \n",
    "    Output\n",
    "    ----------\n",
    "        x: downsampled patches.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, num_patch, embed_dim, name=''):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # A linear transform that doubles the channels \n",
    "        self.linear_trans = Dense(2*embed_dim, use_bias=False, name='{}_linear_trans'.format(name))\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        H, W = self.num_patch\n",
    "        B, L, C = x.get_shape().as_list()\n",
    "        \n",
    "        assert (L == H * W), 'input feature has wrong size'\n",
    "        assert (H % 2 == 0 and W % 2 == 0), '{}-by-{} patches received, they are not even.'.format(H, W)\n",
    "        \n",
    "        # Convert the patch sequence to aligned patches\n",
    "        x = tf.reshape(x, shape=(-1, H, W, C))\n",
    "        \n",
    "        # Downsample\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
    "        \n",
    "        # Convert to the patch squence\n",
    "        x = tf.reshape(x, shape=(-1, (H//2)*(W//2), 4*C))\n",
    "       \n",
    "        # Linear transform\n",
    "        x = self.linear_trans(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class patch_expanding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_patch, embed_dim, upsample_rate, return_vector=True, name=''):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.upsample_rate = upsample_rate\n",
    "        self.return_vector = return_vector\n",
    "        \n",
    "        # Linear transformations that doubles the channels \n",
    "        self.linear_trans1 = Conv2D(upsample_rate*embed_dim, kernel_size=1, use_bias=False, name='{}_linear_trans1'.format(name))\n",
    "        # \n",
    "        self.linear_trans2 = Conv2D(upsample_rate*embed_dim, kernel_size=1, use_bias=False, name='{}_linear_trans1'.format(name))\n",
    "        self.prefix = name\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        H, W = self.num_patch\n",
    "        B, L, C = x.get_shape().as_list()\n",
    "        \n",
    "        assert (L == H * W), 'input feature has wrong size'\n",
    "\n",
    "        x = tf.reshape(x, (-1, H, W, C))\n",
    "        \n",
    "        x = self.linear_trans1(x)\n",
    "        \n",
    "        # rearange depth to number of patches\n",
    "        x = tf.nn.depth_to_space(x, self.upsample_rate, data_format='NHWC', name='{}_d_to_space'.format(self.prefix))\n",
    "        \n",
    "        if self.return_vector:\n",
    "            # Convert aligned patches to a patch sequence\n",
    "            x = tf.reshape(x, (-1, L*self.upsample_rate*self.upsample_rate, C//2))\n",
    "\n",
    "        return x\n",
    "\n",
    "def residual_block(input_x, input_filters, is_downpooling=False, is_uppooling=False):\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(input_x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    " \n",
    "    x = tf.keras.layers.Conv2D(filters=input_filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    " \n",
    "    x = tf.keras.layers.Conv2D(filters=input_filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "    x = tf.keras.layers.concatenate([x, input_x],axis=-1)\n",
    "    x = tf.keras.layers.Conv2D(filters=input_filters, kernel_size=3, strides=1, padding='same')(x)\n",
    "    if is_downpooling:\n",
    "        x = tf.keras.layers.MaxPool2D(pool_size=(2, 2),strides=(2, 2), padding='valid')(x)\n",
    "    if is_uppooling:\n",
    "        x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def Mamba_unet(input_tensor, filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, n_labels, name='Mamba_unet'):\n",
    "    '''\n",
    "    The base of Mamba_unet.\n",
    "    \n",
    "    The general structure:\n",
    "    \n",
    "    1. Input image --> a sequence of patches --> tokenize these patches\n",
    "    2. Downsampling: swin-transformer --> patch merging (pooling)\n",
    "    3. Upsampling: concatenate --> swin-transfprmer --> patch expanding (unpooling)\n",
    "    4. Model head\n",
    "    \n",
    "    '''\n",
    "    # Compute number be patches to be embeded\n",
    "    input_size = input_tensor.shape.as_list()[1:]\n",
    "    num_patch_x = input_size[0]//patch_size[0]\n",
    "    num_patch_y = input_size[1]//patch_size[1]\n",
    "    \n",
    "    # Number of Embedded dimensions\n",
    "    embed_dim = filter_num_begin\n",
    "    \n",
    "    depth_ = depth\n",
    "    \n",
    "    X_skip = []\n",
    "\n",
    "    X = input_tensor\n",
    "    \n",
    "    Img_X = X #图像输入\n",
    "    \n",
    "    X_imageskip=[]\n",
    "    \n",
    "    # Patch extraction\n",
    "    X = patch_extract(patch_size)(X)\n",
    "\n",
    "    # Embed patches to tokens\n",
    "    X = patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "    \n",
    "    # The first Mamba stack\n",
    "    X = ResidualBlock(embed_dim, expand = 2, bias = False, d_conv = 4, conv_bias = True, d_state = 16)(X)                               \n",
    "                                      \n",
    "    X_skip.append(X)\n",
    "    \n",
    "    Img_Xd = residual_block(Img_X,32,is_downpooling=False, is_uppooling=False)\n",
    "    \n",
    "    X_imageskip.append(Img_Xd)\n",
    "    \n",
    "    # Downsampling blocks\n",
    "    for i in range(depth_-1):\n",
    "        \n",
    "        # Patch merging\n",
    "        X = patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "        \n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim*2\n",
    "        num_patch_x = num_patch_x//2\n",
    "        num_patch_y = num_patch_y//2\n",
    "                                      \n",
    "        Img_X = residual_block(Img_X,64*(2*i+1), is_downpooling=True, is_uppooling=False)\n",
    "        X_imageskip.append(Img_X)\n",
    "        Img_X_patch = patch_extract(patch_size)(Img_X)\n",
    "        Img_X_patch = patch_embedding(num_patch_x*num_patch_y, embed_dim)(Img_X_patch)\n",
    "        \n",
    "\n",
    "        #Mamba block\n",
    "        X1 = ResidualBlock(embed_dim, expand = 2, bias = False, d_conv = 2, conv_bias = True, d_state = 16)(X)\n",
    "        X = tf.concat([X1,Img_X_patch],axis=-1)\n",
    "        X = Dense(embed_dim, use_bias=False)(X)\n",
    "        \n",
    "        # Store tensors for concat\n",
    "        X_skip.append(X)\n",
    "        \n",
    "    # reverse indexing encoded tensors and hyperparams\n",
    "    X_skip = X_skip[::-1]\n",
    "    num_heads = num_heads[::-1]\n",
    "    window_size = window_size[::-1]\n",
    "    X_imageskip = X_imageskip[::-1]\n",
    "    \n",
    "    # upsampling begins at the deepest available tensor\n",
    "    X = X_skip[0]\n",
    "    \n",
    "    # other tensors are preserved for concatenation\n",
    "    X_decode = X_skip[1:]\n",
    "    \n",
    "    depth_decode = len(X_decode)\n",
    "    \n",
    "    X_pyramid = []\n",
    "    \n",
    "    for i in range(depth_decode):\n",
    "        \n",
    "        # Patch expanding\n",
    "        X = patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                               embed_dim=embed_dim, \n",
    "                                               upsample_rate=2, \n",
    "                                               return_vector=True)(X)\n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim//2\n",
    "        num_patch_x = num_patch_x*2\n",
    "        num_patch_y = num_patch_y*2\n",
    "        \n",
    "        Img_X = residual_block(tf.concat([Img_X,X_imageskip[i]],axis=-1),embed_dim,is_downpooling=False, is_uppooling=True) #图像上采样\n",
    "        #print(Img_X.shape)\n",
    "        \n",
    "        # Concatenation and linear projection\n",
    "        X = tf.keras.layers.concatenate([X, X_decode[i]], axis=-1, name='{}_concat_{}'.format(name, i))\n",
    "        X = Dense(embed_dim, use_bias=False, name='{}_concat_linear_proj_{}'.format(name, i))(X)\n",
    "\n",
    "        X = ResidualBlock(embed_dim, expand = 2, bias = False, d_conv = 2, conv_bias = True, d_state = 16)(X)\n",
    "        #linear projection\n",
    "        X = Dense(embed_dim, use_bias=False)(X)\n",
    "        #Token转image\n",
    "        X_pyramid_image =  patch_expanding(num_patch=(num_patch_x, num_patch_y), \n",
    "                                           embed_dim=embed_dim, \n",
    "                                           upsample_rate=patch_size[0], \n",
    "                                           return_vector=False)(X)\n",
    "        #图像融合\n",
    "        Img_X = residual_block(tf.concat([Img_X,X_pyramid_image],axis=-1),64*(2*(depth_decode-i)),is_downpooling=False, is_uppooling=False)\n",
    "        X_pyramid.append(Img_X)\n",
    "\n",
    "    for i in range(len(X_pyramid)):\n",
    "        X_pyramid[i]  = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same',activation=\"relu\")(X_pyramid[i])\n",
    "        X_pyramid[i]  = tf.keras.layers.Conv2DTranspose(32, kernel_size=1, strides=2**(2-i), padding='same',activation=\"relu\")(X_pyramid[i])\n",
    "        #print(X_pyramid[i].shape)\n",
    "        #X_pyramid[i] = Conv2D(n_labels, kernel_size=1, use_bias=False, activation='softmax')(X_pyramid[i])\n",
    "\n",
    "    Final_output = tf.keras.layers.concatenate([X_pyramid[0],X_pyramid[1],X_pyramid[2]],axis=-1)\n",
    "    Final_output = Conv2D(16, kernel_size=2, padding='same',activation='relu')(Final_output)\n",
    "    Final_output = Conv2D(n_labels, kernel_size=1,padding='same', activation='softmax')(Final_output)\n",
    "    \n",
    "    return Final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_num_begin = 64      # number of channels in the first downsampling block; it is also the number of embedded dimensions\n",
    "depth = 4                  # the depth of Mamba; depth=4 means three down/upsampling levels and a bottom level \n",
    "stack_num_down = 2         # number of Mambaper downsampling level\n",
    "stack_num_up = 2           # number of Mamba per upsampling level\n",
    "patch_size = (4, 4)        # Extract 4-by-4 patches from the input image. Height and width of the patch must be equal.\n",
    "num_heads = [4, 8, 8, 8]   # number of attention heads per down/upsampling level\n",
    "window_size = [4, 2, 2, 2] # the size of attention window per down/upsampling level\n",
    "num_mlp = 128              # number of MLP nodes within the Transformer\n",
    "\n",
    "n_labels = 2\n",
    "IN = tf.keras.layers.Input((512,512,3))\n",
    "# # Output section\n",
    "OUT = Mamba_unet(IN, filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp,n_labels, \n",
    "                      name='Mamba_unet')\n",
    "# Model configuration\n",
    "model = Model(inputs=[IN,], outputs=OUT)\n",
    "img = tf.random.normal(shape=[1,512,512,3])\n",
    "preds = model(img)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b478914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########加载数据#############\n",
    "image_dir =r'D:\\AI in NTU\\Tunnel segment\\concreteCrackSegmentationDataset\\rgb'\n",
    "mask_dir =r'D:\\AI in NTU\\Tunnel segment\\concreteCrackSegmentationDataset\\BW'\n",
    "image_test_dir =r'D:\\AI in NTU\\Tunnel segment\\concreteCrackSegmentationDataset\\Test_rgb'\n",
    "mask_test_dir =r'D:\\AI in NTU\\Tunnel segment\\concreteCrackSegmentationDataset\\Test_BW'\n",
    "def get_image_and_mask_paths(image_dir:str,mask_dir:str):\n",
    "    '''\n",
    "    获取所有图片与对应标签的路径 \n",
    "    '''\n",
    "    # 导入数据集\n",
    "    all_file_image=os.listdir(image_dir)\n",
    "    all_image=[]\n",
    "    all_mask=[]\n",
    "    for i in range(len(os.listdir(image_dir))):\n",
    "        if (os.path.splitext(all_file_image[i])[1] == \".jpg\") or (os.path.splitext(all_file_image[i])[1] == \".JPG\"):\n",
    "            all_image.append(image_dir + \"\\\\\" + all_file_image[i])\n",
    "            all_mask.append(mask_dir + \"\\\\\" +  all_file_image[i])\n",
    "    all_image=np.array(all_image)[:,np.newaxis]\n",
    "    all_mask =np.array(all_mask)[:,np.newaxis]\n",
    "    all_path = np.concatenate((all_image,all_mask),axis=-1)\n",
    "    return all_path\n",
    "\n",
    "paths = get_image_and_mask_paths(image_dir,mask_dir)\n",
    "validpaths = get_image_and_mask_paths(image_test_dir,mask_test_dir)\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "#创建图片路径及其数字标签的dataset\n",
    "db_train= tf.data.Dataset.from_tensor_slices(paths)\n",
    "db_train = db_train.shuffle(buffer_size=4,seed=2024)\n",
    "db_train = db_train.batch(BATCH_SIZE)\n",
    "\n",
    "db_test= tf.data.Dataset.from_tensor_slices(validpaths)\n",
    "db_test = db_test.shuffle(buffer_size=4,seed=2024)\n",
    "db_test = db_test.batch(BATCH_SIZE)\n",
    "\n",
    "def load_image(path,ran):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.cast(tf.image.decode_jpeg(image,channels=3),dtype=tf.float32)\n",
    "    if ran>0.5:#随机左右翻转图像，增强数据\n",
    "        image=tf.image.flip_left_right(image)\n",
    "    image = tf.image.resize(image,[512,512])\n",
    "    image /= 255.0\n",
    "    image = image*2-1#将整张图片的值规范在[-1,1]之间\n",
    "    return image\n",
    "\n",
    "def load_image_mask(path,ran):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.cast(tf.image.decode_jpeg(image,channels=1),dtype=tf.float32)\n",
    "    if ran>0.5:#随机左右翻转图像，增强数据\n",
    "        image=tf.image.flip_left_right(image)\n",
    "    image /= 255.0\n",
    "    image = tf.image.resize(image,[512,512])\n",
    "    image = tf.cast(tf.round(image),dtype=tf.uint8)\n",
    "    image = tf.one_hot(image,depth=2)\n",
    "    return image\n",
    "\n",
    "#####训练##########\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "History=[]\n",
    "valid_num=0\n",
    "for epoch in range(0,100,1):\n",
    "    count=0\n",
    "    count_valid=0\n",
    "    Average_loss=0\n",
    "    Average_valid_MeanIoU=0\n",
    "    for batch_size in db_train:\n",
    "        count+=1\n",
    "        train_image = []\n",
    "        mask_image = []\n",
    "        for i in range(len(batch_size)):\n",
    "            ran = tf.random.uniform(())\n",
    "            train_image.append(load_image(batch_size[i][0],ran))\n",
    "            mask = load_image_mask(batch_size[i][1],ran)\n",
    "            mask_image.append(mask)\n",
    "        mask_image =np.squeeze((mask_image))\n",
    "        train_image=np.array(train_image)\n",
    "\n",
    "        #计算损失\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted_image = model(train_image) \n",
    "            loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(mask_image,predicted_image))\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "        Average_loss = Average_loss + loss\n",
    "        \n",
    "    Average_loss = Average_loss/count\n",
    "    \n",
    "    for batch_size in db_test:\n",
    "        count_valid+=1\n",
    "        valid_image = []\n",
    "        valid_mask_image = []\n",
    "        for i in range(len(batch_size)):\n",
    "            ran = tf.random.uniform(())\n",
    "            valid_image.append(load_image(batch_size[i][0],ran))\n",
    "            validmask = load_image_mask(batch_size[i][1],ran)\n",
    "            valid_mask_image.append(validmask)\n",
    "        valid_mask_image =np.squeeze((valid_mask_image))\n",
    "        valid_image = np.array(valid_image)\n",
    "        predicted_image = model(valid_image) \n",
    "        m = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "        m.update_state(tf.argmax(valid_mask_image,axis=-1),tf.argmax(predicted_image,axis=-1))\n",
    "        Average_valid_MeanIoU+=tf.reduce_mean(m.result().numpy())\n",
    "    Average_valid_MeanIoU = Average_valid_MeanIoU/count_valid\n",
    "    History.append([epoch, Average_loss,Average_valid_MeanIoU])\n",
    "    tf.print(\"=>Epoch%4d  Averageloss:%4.6f Average_valid_MeanIoU:%4.6f\" %(epoch, Average_loss,Average_valid_MeanIoU))   \n",
    "    if Average_valid_MeanIoU>valid_num:\n",
    "        valid_num = Average_valid_MeanIoU\n",
    "        model.save_weights(\"./MambaUnet_CCSD_crop_pyramid(4×4).h5\")\n",
    "            \n",
    "    plt.imshow(valid_image[0])\n",
    "    plt.show()\n",
    "    plt.imshow(np.argmax(predicted_image,axis=-1)[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c13d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################测试###############\n",
    "#automatic_gpu_usage()#分配GPU\n",
    "###########sample##################\n",
    "BATCH_SIZE = 2\n",
    "##########加载数据#############\n",
    "image_test_dir =r'D:\\AI in NTU\\Tunnel segment\\concreteCrackSegmentationDataset\\Test_rgb'\n",
    "mask_test_dir =r'D:\\AI in NTU\\Tunnel segment\\concreteCrackSegmentationDataset\\Test_BW'\n",
    "def get_image_and_mask_paths(image_dir:str,mask_dir:str):\n",
    "    '''\n",
    "    获取所有图片与对应标签的路径 \n",
    "    '''\n",
    "    # 导入数据集\n",
    "    all_file_image=os.listdir(image_dir)\n",
    "    all_image=[]\n",
    "    all_mask=[]\n",
    "    for i in range(len(os.listdir(image_dir))):\n",
    "        if (os.path.splitext(all_file_image[i])[1] == \".jpg\") or (os.path.splitext(all_file_image[i])[1] == \".JPG\"):\n",
    "            all_image.append(image_dir + \"\\\\\" + all_file_image[i])\n",
    "            all_mask.append(mask_dir + \"\\\\\" +  all_file_image[i])\n",
    "    all_image=np.array(all_image)[:,np.newaxis]\n",
    "    all_mask =np.array(all_mask)[:,np.newaxis]\n",
    "    all_path = np.concatenate((all_image,all_mask),axis=-1)\n",
    "    return all_path\n",
    "\n",
    "paths = get_image_and_mask_paths(image_test_dir,mask_test_dir)\n",
    "\n",
    "#创建图片路径及其数字标签的dataset\n",
    "db_train= tf.data.Dataset.from_tensor_slices(paths)\n",
    "db_train = db_train.shuffle(buffer_size=4,seed=2023)\n",
    "db_train = db_train.batch(BATCH_SIZE)\n",
    "\n",
    "def load_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.cast(tf.image.decode_jpeg(image,channels=3),dtype=tf.float32)\n",
    "    image = tf.image.resize(image,[512,512])\n",
    "    image /= 255.0\n",
    "    image = image*2-1#将整张图片的值规范在[-1,1]之间\n",
    "    return image\n",
    "\n",
    "def load_image_mask(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.cast(tf.image.decode_jpeg(image,channels=1),dtype=tf.float32)\n",
    "    image /= 255.0\n",
    "    image = tf.image.resize(image,[512,512])\n",
    "    image = tf.cast(tf.round(image),dtype=tf.uint8)\n",
    "    image = tf.one_hot(image,depth=2)\n",
    "    return image\n",
    "\n",
    "#Network \n",
    "model.load_weights(\"./MambaUnet_CCSD_crop_pyramid(4×4).h5\")\n",
    "\n",
    "count=0\n",
    "IoU_list=[]\n",
    "Precision=[]\n",
    "Recall=[]\n",
    "Accuracy=[]\n",
    "MAE=[]\n",
    "F1=[]\n",
    "for batch_size in db_train:\n",
    "    count+=1\n",
    "    train_image = []\n",
    "    mask_image = []\n",
    "    for i in range(len(batch_size)):\n",
    "        train_image.append(load_image(batch_size[i][0]))\n",
    "        mask_image.append(load_image_mask(batch_size[i][1]))\n",
    "    mask_image =np.squeeze((mask_image))\n",
    "    train_image=np.array(train_image)\n",
    "    predicted_image = model(train_image)\n",
    "    segement_image=np.argmax(predicted_image,axis=-1)\n",
    "    mask_image_r = np.argmax(mask_image,axis=-1)\n",
    "    print(train_image.shape)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        plt.imshow(train_image[i])\n",
    "        plt.show()\n",
    "        plt.imshow(segement_image[i])\n",
    "        plt.show()\n",
    "        m = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "        m.update_state(mask_image_r[i],segement_image[i])\n",
    "        a = tf.keras.metrics.Accuracy()\n",
    "        a.update_state(mask_image_r[i],segement_image[i])\n",
    "        p = tf.keras.metrics.Precision()\n",
    "        p.update_state(mask_image_r[i],segement_image[i])\n",
    "        r = tf.keras.metrics.Recall()\n",
    "        r.update_state(mask_image_r[i],segement_image[i])\n",
    "        mae = tf.reduce_sum(tf.math.abs(mask_image_r[i]-segement_image[i]))/512/512\n",
    "        #print(mae)\n",
    "        IoU_list.append(m.result().numpy())\n",
    "        Accuracy.append(a.result().numpy())\n",
    "        Precision.append(p.result().numpy())\n",
    "        Recall.append(r.result().numpy())\n",
    "        MAE.append(mae)\n",
    "        F1.append(2 * (p.result().numpy() * r.result().numpy()) / (p.result().numpy() + r.result().numpy()))\n",
    "        \n",
    "print(np.average(IoU_list))\n",
    "print(np.average(Accuracy))\n",
    "print(np.average(Precision))\n",
    "print(np.average(Recall))\n",
    "print(np.average(F1))\n",
    "print(np.average(MAE))\n",
    "plt.bar(range(len(IoU_list)), IoU_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c7edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
